{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b682b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dakot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dakot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dakot\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Import data\n",
    "import json;\n",
    "#targets\n",
    "#abstract\n",
    "f = open(\"../data/News_Category_Dataset_v2.json\",\"r\");\n",
    "\n",
    "text = f.read()\n",
    "\n",
    "textarr = text.splitlines();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b181929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'ENTERTAINMENT', 'headline': 'Hugh Grant Marries For The First Time At Age 57', 'authors': 'Ron Dicker', 'link': 'https://www.huffingtonpost.com/entry/hugh-grant-marries_us_5b09212ce4b0568a880b9a8c', 'short_description': 'The actor and his longtime girlfriend Anna Eberstein tied the knot in a civil ceremony.', 'date': '2018-05-26'}\n"
     ]
    }
   ],
   "source": [
    "jsonarr = []\n",
    "targets = []\n",
    "headlines = []\n",
    "abstracts = []\n",
    "\n",
    "#Make data into workable arrays\n",
    "i=0;\n",
    "\n",
    "\n",
    "for x in textarr:\n",
    "    x_type=json.loads(x)[\"category\"];\n",
    "    \n",
    "    category=\"\";\n",
    "    jsonarr.append(json.loads(x));\n",
    "    #if x_type==\"PARENTING\":\n",
    "        #print(json.loads(x));\n",
    "        #targets.append(jsonarr[i][\"category\"])\n",
    "        #abstracts.append(jsonarr[i][\"short_description\"])\n",
    "        #headlines.append(jsonarr[i][\"headline\"])\n",
    "    if x_type == \"POLITICS\" or x_type == \"QUEER VOICES\" or x_type == \"BLACK VOICES\" or x_type == \"LATINO VOICES\" or x_type == \"WOMEN\" or x_type ==\"ENVIRONMENT\":\n",
    "        category=\"POLITICS\";\n",
    "    elif x_type == \"WELLNESS\" or x_type == \"HEALTHY LIVING\":\n",
    "        category=\"HEALTH\";\n",
    "    elif x_type == \"WEDDINGS\" or x_type == \"ENTERTAINMENT\" or x_type ==\"STYLE & BEAUTY\" or x_type == \"COMEDY\" or x_type ==\"SPORTS\" or x_type == \"MEDIA\" or x_type==\"STYLE\" or x_type ==\"TASTE\" or x_type == \"CULTURE & ARTS\" or x_type==\"ARTS\" or x_type ==\"DIVORCE\" or x_type == \"IMPACT\" :\n",
    "        category=\"ENTERTAINMENT\";\n",
    "    elif x_type ==\"THE WORLDPOST\" or x_type == \"WORLDPOST\" or x_type ==\"CRIME\" or x_type==\"WEIRD NEWS\" or x_type==\"WORLD NEWS\" or x_type==\"GOOD NEWS\":\n",
    "        category=\"NEWS\"\n",
    "    else:\n",
    "        category=\"INFORMATION\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    targets.append(category);\n",
    "    abstracts.append(jsonarr[i][\"short_description\"])\n",
    "    headlines.append(jsonarr[i][\"headline\"])\n",
    "    i+=1;\n",
    "print(jsonarr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a26fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 type                                           headline  \\\n",
      "0                NEWS  There Were 2 Mass Shootings In Texas Last Week...   \n",
      "1       ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
      "2       ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
      "3       ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
      "4       ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
      "...               ...                                                ...   \n",
      "200848    INFORMATION  RIM CEO Thorsten Heins' 'Significant' Plans Fo...   \n",
      "200849  ENTERTAINMENT  Maria Sharapova Stunned By Victoria Azarenka I...   \n",
      "200850  ENTERTAINMENT  Giants Over Patriots, Jets Over Colts Among  M...   \n",
      "200851  ENTERTAINMENT  Aldon Smith Arrested: 49ers Linebacker Busted ...   \n",
      "200852  ENTERTAINMENT  Dwight Howard Rips Teammates After Magic Loss ...   \n",
      "\n",
      "                                                 abstract  \n",
      "0       She left her husband. He killed their children...  \n",
      "1                                Of course it has a song.  \n",
      "2       The actor and his longtime girlfriend Anna Ebe...  \n",
      "3       The actor gives Dems an ass-kicking for not fi...  \n",
      "4       The \"Dietland\" actress said using the bags is ...  \n",
      "...                                                   ...  \n",
      "200848  Verizon Wireless and AT&T are already promotin...  \n",
      "200849  Afterward, Azarenka, more effusive with the pr...  \n",
      "200850  Leading up to Super Bowl XLVI, the most talked...  \n",
      "200851  CORRECTION: An earlier version of this story i...  \n",
      "200852  The five-time all-star center tore into his te...  \n",
      "\n",
      "[200853 rows x 3 columns]\n",
      "ENTERTAINMENT    56006\n",
      "INFORMATION      54910\n",
      "POLITICS         49523\n",
      "HEALTH           24521\n",
      "NEWS             15893\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#turn arrays into real dataframe\n",
    "arr = np.asarray([targets,headlines,abstracts]).T\n",
    "df = pd.DataFrame(arr,columns=[\"type\",\"headline\",\"abstract\"]);\n",
    "\n",
    "#print(arr)\n",
    "print(df)\n",
    "print(df[\"type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62e0b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 type                                           headline  \\\n",
      "0                NEWS  there were  mass shootings in texas last week ...   \n",
      "1       ENTERTAINMENT  will smith joins diplo and nicky jam for the  ...   \n",
      "2       ENTERTAINMENT      hugh grant marries for the first time at age    \n",
      "3       ENTERTAINMENT  jim carrey blasts castrato adam schiff and dem...   \n",
      "4       ENTERTAINMENT  julianna margulies uses donald trump poop bags...   \n",
      "...               ...                                                ...   \n",
      "200848    INFORMATION  rim ceo thorsten heins significant plans for b...   \n",
      "200849  ENTERTAINMENT  maria sharapova stunned by victoria azarenka i...   \n",
      "200850  ENTERTAINMENT  giants over patriots jets over colts among  mo...   \n",
      "200851  ENTERTAINMENT  aldon smith arrested ers linebacker busted for...   \n",
      "200852  ENTERTAINMENT  dwight howard rips teammates after magic loss ...   \n",
      "\n",
      "                                                 abstract  \n",
      "0       she left her husband he killed their children ...  \n",
      "1                                 of course it has a song  \n",
      "2       the actor and his longtime girlfriend anna ebe...  \n",
      "3       the actor gives dems an ass-kicking for not fi...  \n",
      "4       the dietland actress said using the bags is a ...  \n",
      "...                                                   ...  \n",
      "200848  verizon wireless and att are already promoting...  \n",
      "200849  afterward azarenka more effusive with the pres...  \n",
      "200850  leading up to super bowl xlvi the most talked ...  \n",
      "200851  correction an earlier version of this story in...  \n",
      "200852  the five-time all-star center tore into his te...  \n",
      "\n",
      "[200853 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#Clean up our data\n",
    "def clean_text(text):\n",
    "    rem = re.compile(r'')\n",
    "    rem2 = re.compile(r'[^a-zA-Z \\-]')\n",
    "    text = re.sub(rem,'',text)\n",
    "    text = re.sub(rem2,'',text)\n",
    "    return text.lower()\n",
    "\n",
    "df[\"headline\"] = df[\"headline\"].apply(clean_text);\n",
    "df[\"abstract\"] = df[\"abstract\"].apply(clean_text);\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9a58a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 type                                           headline  \\\n",
      "0                NEWS                  mass shootings texas last week tv   \n",
      "1       ENTERTAINMENT   smith joins diplo nicky jam world cups offici...   \n",
      "2       ENTERTAINMENT                  hugh grant marries first time age   \n",
      "3       ENTERTAINMENT   jim carrey blasts castrato adam schiff democr...   \n",
      "4       ENTERTAINMENT   julianna margulies uses donald trump poop bag...   \n",
      "...               ...                                                ...   \n",
      "200848    INFORMATION   rim ceo thorsten heins significant plans blac...   \n",
      "200849  ENTERTAINMENT   maria sharapova stunned victoria azarenka aus...   \n",
      "200850  ENTERTAINMENT   giants patriots jets colts among improbable s...   \n",
      "200851  ENTERTAINMENT     aldon smith arrested ers linebacker busted dui   \n",
      "200852  ENTERTAINMENT    dwight howard rips teammates magic loss hornets   \n",
      "\n",
      "                                                 abstract  \n",
      "0        left husband killed children another day america  \n",
      "1                                             course song  \n",
      "2        actor longtime girlfriend anna eberstein tied...  \n",
      "3        actor gives dems ass-kicking fighting hard en...  \n",
      "4        dietland actress said using bags really catha...  \n",
      "...                                                   ...  \n",
      "200848   verizon wireless att already promoting lte de...  \n",
      "200849   afterward azarenka effusive press normal cred...  \n",
      "200850   leading super bowl xlvi talked game could end...  \n",
      "200851   correction earlier version story incorrectly ...  \n",
      "200852   five-time all-star center tore teammates frid...  \n",
      "\n",
      "[200853 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def cleanup_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #print(text)\n",
    "    rem = re.compile(r' +')\n",
    "    text = re.sub(rem,' ',text)\n",
    "    textarr = text.strip().split(' ');\n",
    "    #print(textarr)\n",
    "    new_text=\"\"\n",
    "    for x in textarr:\n",
    "        if x not in stop_words:\n",
    "            new_text=new_text+ \" \"+x\n",
    "        #else:\n",
    "            #print(\"excluded: \" +x)\n",
    "    return new_text;\n",
    "\n",
    "df[\"headline\"] = df[\"headline\"].apply(cleanup_stopwords);\n",
    "df[\"abstract\"] = df[\"abstract\"].apply(cleanup_stopwords);\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a7d268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 type                                           headline  \\\n",
      "0                NEWS                   mass shooting texas last week tv   \n",
      "1       ENTERTAINMENT   smith join diplo nicky jam world cup official...   \n",
      "2       ENTERTAINMENT                  hugh grant marries first time age   \n",
      "3       ENTERTAINMENT   jim carrey blast castrato adam schiff democra...   \n",
      "4       ENTERTAINMENT   julianna margulies us donald trump poop bag p...   \n",
      "...               ...                                                ...   \n",
      "200848    INFORMATION   rim ceo thorsten heins significant plan black...   \n",
      "200849  ENTERTAINMENT   maria sharapova stunned victoria azarenka aus...   \n",
      "200850  ENTERTAINMENT   giant patriot jet colt among improbable super...   \n",
      "200851  ENTERTAINMENT      aldon smith arrested er linebacker busted duo   \n",
      "200852  ENTERTAINMENT       dwight howard rip teammate magic loss hornet   \n",
      "\n",
      "                                                 abstract  \n",
      "0           left husband killed child another day america  \n",
      "1                                             course song  \n",
      "2        actor longtime girlfriend anna eberstein tied...  \n",
      "3        actor give dems ass-kicking fighting hard eno...  \n",
      "4        dietland actress said using bag really cathar...  \n",
      "...                                                   ...  \n",
      "200848   verizon wireless att already promoting lte de...  \n",
      "200849   afterward azarenka effusive press normal cred...  \n",
      "200850   leading super bowl xlvi talked game could end...  \n",
      "200851   correction earlier version story incorrectly ...  \n",
      "200852   five-time all-star center tore teammate frida...  \n",
      "\n",
      "[200853 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    textarr = text.strip().split(' ');\n",
    "    newtext=\"\"\n",
    "    for word in textarr:\n",
    "        newtext=newtext+\" \"+net.lemmatize(word)\n",
    "    return newtext\n",
    "\n",
    "df[\"headline\"] = df[\"headline\"].apply(lemmatize);\n",
    "df[\"abstract\"] = df[\"abstract\"].apply(lemmatize);\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05fa75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split our data into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d323ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct countvectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "x_train_counts=count_vect.fit_transform(train[\"headline\"])\n",
    "#x_train_counts=count_vect.fit_transform(np.asarray(train.abstract));\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(x_train_counts)\n",
    "x_train_tf = tf_transformer.transform(x_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a049443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test multifeature model\n",
    "\n",
    "\n",
    "#count_vect1 = CountVectorizer()\n",
    "#count_vect2 = CountVectorizer()\n",
    "    \n",
    "#headline_count = count_vect1.fit_transform(np.asarray(train.headline))\n",
    "#abstract_count = count_vect2.fit_transform(np.asarray(train.abstract))\n",
    "#tf_transformer1 = TfidfTransformer(use_idf=True,smooth_idf=False).fit(headline_count)\n",
    "#tf_transformer2 = TfidfTransformer(use_idf=True,smooth_idf=False).fit(abstract_count)\n",
    "    \n",
    "#feature1 = tf_transformer1.transform(headline_count)\n",
    "#feature2 = tf_transformer2.transform(abstract_count)\n",
    "#featureset = np.asarray([feature1,feature2])\n",
    "#print(featureset)\n",
    "\n",
    "\n",
    "#model = MultinomialNB(alpha=.5,fit_prior=False).fit(featureset,np.asarray(train.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd09a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train ourmodel\n",
    "#ourmodel = MultinomialNB().fit(x_train_tf,y=np.asarray(train.type))\n",
    "#svm = SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3,random_state=42,max_iter=10,tol=None).fit(x_train_tf,y=np.asarray(train.type))\n",
    "#nn = MLPClassifier(max_iter=10).fit(x_train_tf,y=np.asarray(train.type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d964168c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()),\n",
       "                ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
       "                ('clf',\n",
       "                 MLPClassifier(early_stopping=True, hidden_layer_sizes=(20,),\n",
       "                               max_iter=50, random_state=42))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#class_weights = dict(zip(np.unique(np.asarray(train.type)),\n",
    "#                     class_weight.compute_class_weight(\n",
    "#                         'balanced',classes=np.unique(np.asarray(train.type)),y=np.asarray(train.type))))\n",
    "#print(len(class_weights))\n",
    "\n",
    "#make pipelines\n",
    "ourmodel = Pipeline([\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer(use_idf=True)),\n",
    "    ('clf',MultinomialNB(fit_prior=False)),\n",
    "])\n",
    "\n",
    "svm = Pipeline([\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer(use_idf=True)),\n",
    "    ('clf',SGDClassifier(loss='perceptron', penalty='l2',\n",
    "                         alpha=1e-2,max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "nn =  Pipeline([\n",
    "    ('vect',CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer(use_idf=True,smooth_idf=False)),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(20,),\n",
    "                          early_stopping=True,max_iter=50,random_state=42),\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "#ourmodel.fit(np.asarray(train.headline),np.asarray(train.type)) \n",
    "ourmodel.fit(train[\"headline\"],np.asarray(train.type))\n",
    "svm.fit(np.asarray(train.headline),np.asarray(train.type))\n",
    "nn.fit(np.asarray(train.headline),np.asarray(train.type))\n",
    "#ourmodel.fit(np.asarray(train.abstract),np.asarray(train.type))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "44b5a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model\n",
    "\n",
    "#x_test_counts = count_vect.transform(np.asarray(test.headline));\n",
    "#x_test_counts = count_vect.transform(np.asarray(test.abstract));\n",
    "#x_test_tf = tf_transformer.transform(x_test_counts);\n",
    "\n",
    "test_predicted = ourmodel.predict(np.asarray(test.headline));\n",
    "train_predicted = ourmodel.predict(np.asarray(train.headline));\n",
    "#test_predicted = ourmodel.predict(np.asarray(test.abstract));\n",
    "#train_predicted = ourmodel.predict(np.asarray(train.abstract));\n",
    "test_answer = np.asarray(test.type)\n",
    "train_answer = np.asarray(train.type)\n",
    "\n",
    "\n",
    "svm_test_predicted = svm.predict(np.asarray(test.headline));\n",
    "svm_train_predicted = svm.predict(np.asarray(train.headline));\n",
    "svm_test_answer = np.asarray(test.type)\n",
    "svm_train_answer = np.asarray(train.type)\n",
    "\n",
    "nn_test_predicted = nn.predict(np.asarray(test.headline));\n",
    "nn_train_predicted = nn.predict(np.asarray(train.headline));\n",
    "nn_test_answer = np.asarray(test.type)\n",
    "nn_train_answer = np.asarray(train.type)\n",
    "#print(train_answer)\n",
    "#print(train_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0009635a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes Train: 0.7451012468260347\n",
      "Naive bayes Test: 0.6894915029208709\n",
      "SVM Train: 0.7881889371750465\n",
      "SVM Test: 0.6452801380775358\n",
      "NN Train: 0.8040925482051537\n",
      "NN Test: 0.7119622942113648\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Naive bayes Train: \"+str(accuracy_score(train_answer,train_predicted)))\n",
    "print(\"Naive bayes Test: \"+str(accuracy_score(test_answer,test_predicted)))\n",
    "print(\"SVM Train: \"+str(accuracy_score(svm_train_answer,svm_train_predicted)))\n",
    "print(\"SVM Test: \"+str(accuracy_score(svm_test_answer,svm_test_predicted)))\n",
    "print(\"NN Train: \"+str(accuracy_score(nn_train_answer,nn_train_predicted)))\n",
    "print(\"NN Test: \"+str(accuracy_score(nn_test_answer,nn_test_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ad4e5e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "ENTERTAINMENT       0.69      0.74      0.72     15711\n",
      "       HEALTH       0.69      0.67      0.68      7608\n",
      "  INFORMATION       0.68      0.67      0.67     16703\n",
      "         NEWS       0.53      0.60      0.57      4218\n",
      "     POLITICS       0.75      0.69      0.72     16016\n",
      "\n",
      "     accuracy                           0.69     60256\n",
      "    macro avg       0.67      0.68      0.67     60256\n",
      " weighted avg       0.69      0.69      0.69     60256\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "ENTERTAINMENT       0.68      0.67      0.68     16857\n",
      "       HEALTH       0.66      0.60      0.63      8171\n",
      "  INFORMATION       0.61      0.64      0.62     15603\n",
      "         NEWS       0.50      0.53      0.52      4521\n",
      "     POLITICS       0.69      0.68      0.68     15104\n",
      "\n",
      "     accuracy                           0.65     60256\n",
      "    macro avg       0.63      0.62      0.63     60256\n",
      " weighted avg       0.65      0.65      0.65     60256\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "ENTERTAINMENT       0.74      0.72      0.73     17157\n",
      "       HEALTH       0.68      0.72      0.70      6936\n",
      "  INFORMATION       0.74      0.66      0.70     18369\n",
      "         NEWS       0.54      0.68      0.60      3734\n",
      "     POLITICS       0.72      0.77      0.74     14060\n",
      "\n",
      "     accuracy                           0.71     60256\n",
      "    macro avg       0.68      0.71      0.69     60256\n",
      " weighted avg       0.72      0.71      0.71     60256\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(test_predicted,test_answer))\n",
    "\n",
    "print(metrics.classification_report(svm_test_predicted,svm_test_answer))\n",
    "\n",
    "print(metrics.classification_report(nn_test_predicted,nn_test_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10bc05b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"naive-bayes.pkl\",\"wb\") as f:\n",
    "    pickle.dump(ourmodel,f)\n",
    "    \n",
    "with open(\"svm.pkl\",\"wb\") as f:\n",
    "    pickle.dump(svm,f)\n",
    "\n",
    "with open(\"mlp.pkl\",\"wb\") as f:\n",
    "    pickle.dump(nn,f)\n",
    "    \n",
    "with open(\"test_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "16ec563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i =0\n",
    "arr=\"[\"\n",
    "while i <=50:\n",
    "    rN = random.randint(0,len(jsonarr));\n",
    "    x = jsonarr[rN]\n",
    "    jsonstr= dict()\n",
    "    jsonstr[\"headline\"] = x[\"headline\"]\n",
    "    jsonstr[\"type\"] = targets[rN];\n",
    "    arr+=json.dumps(jsonstr)+\",\"\n",
    "    i+=1;\n",
    "\n",
    "arr=arr[0:len(arr)-1]+\"]\"\n",
    "\n",
    "f = open(\"testdata.json\",'w');\n",
    "f.write(arr)\n",
    "f.flush()\n",
    "f.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f3552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24721e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
